## Semana 4: Clase 2

YARN		-> Calendariza los procesos (Pone todo en cola)
MESOS		-> Calendarizador, administra los recursos para ejercutar los procesos (Calcula si el proceso se puede ejecutar y no permite ejecutar si no se tienen los recursos suficientes)


Spark Driver(Cluster Manager) -> Contexto (Canales de Conexión entre el nodo maestro y lo Workers)
				Hay un contexto por PROCESO
				Garantiza los límites de recursos que se respetan aún cuando se envíe una instrucción diferente
				a los workers


Map Reduce de Hadoop es diferente al de Spark
las tablas master vienen en formato PARQUET

Se recomienda crear tablas con Spark en lugar de hadoop (Hive, Impala)

Cadausuario tiene perfilado el número de NÚCLEOS y RAM (TALLAS S, M, L XL)

En spark-submit se pueden modificar los valores de la configuración

SANDBOX

Cada área de negocio tiene un espacio en común con recursos compartidos, por lo cual la talla no garantiza los recursos
Se dividen entre Live y Work.



ipython nbconvert --execute --ExecutePreprocessor.timeout=-1 <nombre_archivo.ipyn>


Para el SPARK SUBMIT solamente se mandan .py y .jar (compilado de SCALA)


##Semana 4: Clase 3


Módulo STREAMING en DATIO para obtener datos en tiempo real 
	+ Scrapy - para leer datos de la web
RDD = RESILIANT DISTRIBUTED DATASET, se autoregenera

UNA TRANSFORMACIÓN ES DISTRIBUIDO ES UNA OPERACIÓN MAP TIPICAMENTE
UNA ACCIÓN ES PARALELO O CONCURRENTE ES UNA OPERACIÓN REDUCE TIPICAMENTE

##Semana 4: Clase 4

uname
lscpu

git reset : Se revierten los cambios antes del push 


